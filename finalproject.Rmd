---
title: "Google Play Store Data Analysis Report"
author: "Suheng Yao"
date: "2024-11-18"
output: pdf_document
header-includes:
  - \setkeys{Gin}{width=0.8\textwidth}
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(corrplot)
library(MASS)
options(max.print = 90)
library(lme4)
library(kableExtra)
library(cowplot)
```


```{r, echo=FALSE}
# Load in The Dataset
df <- read.csv("google_play_store_dataset.csv")
```
# Abstract

The main purpose of this report is about prediction of rating scores of the apps in google play store and find out the important variables that can influence the rating score. The method used are multiple linear regression model and linear mixed effect model. After testing for the model results, the multiple linear regression gets the better performance, and the important variables are number of reviews, price, number of installs, size, category and the time the app was last updated. In the discussion section, the report had a literature review, comparing the approach used in this report with methods used by other researchers. Also, some biases within the dataset used is mentioned in the discussion section.  

# Introduction

Google Play Store has always been the most popular and largest app store for Android phone users across the world. Since it is pre-installed on supporting Android devices, and the operative system holds over 70 percent of the global market, Google Play Store becomes the default app hub for most of the Android users worldwide. Up until the second quarter of 2024, there are 2.26 million apps available in the store[1].

In this project, the main question of interest is: What are the important factors that can affect the apps ratings? The reasons why I am interested in this question are divided into two part: first of all, since I use Google Play to install apps every day, I am always curious about what kind of apps can have good ratings scores and why some apps I find easy to use get low rating score. Secondly, by doing this analysis, if I want to build apps to publish on Google Play in the future, then I will know which essential factors to focus on or the popular fields to go into.

This project report will be divided into four parts: data cleaning section will talk about data cleaning process; EDA section will include the graph of variables of interest and initial understanding of the data; methods section will talk about model selection, model building and model output results; discussion section will further analyze those findings and talk about the next steps for the analysis; appendix is the last part, which will raw R model output and references.

# Data Cleaning
```{r, echo=FALSE}
glimpse(df)
```
Looking at the structure of the dataset, there are in total 10,841 observations and 13 variables related to each app. The response variable is **Rating**, which is a continuous variable. By looking at the column names of the data, "Category" and "Genre" seem to have similar data, I can print out the unique values in those two columns:
```{r, echo=FALSE}
print(unique(df$Category))
```
```{r, echo=FALSE}
print(unique(df$Genres))
```
Based on the printed results above, "Genre" is just the more detailed classification of "Category", since in this project, I mostly focus on the general groups of apps, I can remove the "Genre" column and change the Category name to lower case.
```{r, echo=FALSE}
df <- df %>%
  dplyr::select(-Genres)

df$Category <- tolower(df$Category)
```
Also, from the distinct values of category, there is a category called "1.9", which does not make sense. Since there is only one record of data with category "1.9", I can just remove this record from the dataset.
```{r, echo=FALSE}
df <- df %>%
  filter(Category != 1.9)
```
Now, let's check if there are any duplicated apps in the dataset:
```{r, echo=FALSE}
print(sum(duplicated(df$App)))
```
From the analysis above, there are 1181 duplicated apps in the dataset, to make further analysis easier, I will just keep the first occurrence of each app record.
```{r, echo=FALSE}
df <- df %>%
  distinct(App, .keep_all = TRUE)
```
Now, I need to check if there are NA values in the dataset:
```{r, echo=FALSE}
summary(df)
```
From the result shown above, in the response variable "Rating", there are 1463 missing values. To maintain the size of the data, I will fill in those missing values using median values of the "Rating" column.
```{r, echo=FALSE}
df$Rating[is.na(df$Rating)] <- median(df$Rating, na.rm = TRUE)
summary(df)
```
Since it makes more sense to talk about installs, reviews, size and price in numerical values, I will change those variables from categorical to numeric:
```{r, echo=FALSE, warning=FALSE}
df$Reviews <- as.numeric(df$Reviews)
df$Size <- as.numeric(gsub("M", "", df$Size))
colnames(df)[colnames(df) == "Size"] <- "Size(in MB)"
df$Price <- as.numeric(gsub("\\$", "", df$Price))
summary(df)
```
Since Most of the NA values in Size are related to "Varies with Device" value in the original dataset, and in this project, I want to mostly focus on the app with fixed size, I will just remove those app records with varied app sizes.
```{r, echo=FALSE}
df <- df %>%
  filter(!is.na(`Size(in MB)`))
```

Transforming Installs is a more complex matter, I will solve this problem differently. First check the distinct values in the variable "Installs":
```{r, echo=FALSE}
print(unique(df$Installs))
```
To convert those values into numeric values and avoid duplicate values, take "500+" as an example, I will change it to a value between 500 to 1000 because "1000+" will be on a different level.
```{r, echo=FALSE}
df$Installs <- as.numeric(gsub("\\+", "", gsub(",", "", df$Installs)))

generate_installs <- function(x) {
  if (x == 0) {
    return(0)
  } else if (x == 1) {
    lower_bound <- x
    upper_bound <- 10
  } else if (x == 5) {
    lower_bound <- x
    upper_bound <- 10
  } else if (x == 10) {
    lower_bound <- x
    upper_bound <- 50
  } else if (x == 50) {
    lower_bound <- x
    upper_bound <- 100
  } else if (x == 100) {
    lower_bound <- x
    upper_bound <- 500
  } else if (x == 500) {
    lower_bound <- x
    upper_bound <- 1000
  } else if (x == 1000) {
    lower_bound <- x
    upper_bound <- 5000
  } else if (x == 5000) {
    lower_bound <- x
    upper_bound <- 10000
  } else if (x == 10000) {
    lower_bound <- x
    upper_bound <- 50000
  } else if (x == 50000) {
    lower_bound <- x
    upper_bound <- 100000
  } else if (x == 100000) {
    lower_bound <- x
    upper_bound <- 500000
  } else if (x == 500000) {
    lower_bound <- x
    upper_bound <- 1000000
  } else if (x == 1000000) {
    lower_bound <- x
    upper_bound <- 5000000
  } else if (x == 5000000) {
    lower_bound <- x
    upper_bound <- 10000000
  } else if (x == 10000000) {
    lower_bound <- x
    upper_bound <- 50000000
  } else if (x == 50000000) {
    lower_bound <- x
    upper_bound <- 100000000
  } else if (x == 100000000) {
    lower_bound <- x
    upper_bound <- 500000000
  } else if (x == 500000000) {
    lower_bound <- x
    upper_bound <- 1000000000
  } else if (x == 1000000000) {
    lower_bound <- x
    upper_bound <- 2000000000
  } else {
    lower_bound <- x
    upper_bound <- x * 10  # Fallback case if something is missed
  }
    return(round(runif(1, min = lower_bound, max = upper_bound)))
}

df$Installs <- sapply(df$Installs, generate_installs)
summary(df)
```
After those data cleaning is done, I will start doing EDA.

# EDA
## Step 1: Bar Plot Counting Distinct Apps in Each Category
```{r, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
category_counts <- df %>%
  group_by(Category) %>%
  summarise(App_Count = n_distinct(App)) %>%
  arrange(desc(App_Count))

ggplot(category_counts, aes(x = reorder(Category, -App_Count), y = App_Count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(title = "Number of Distinct Apps in Each Category",
       x = "Category",
       y = "Number of Distinct Apps") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
From the count of distinct apps, it is clear that category family gets the most number of apps, with more than 1500 apps in the category. The second most category is game, which is only half the number of the family category, and the third most is the tools category. The category with the least number of distinct apps is comics, with only less than 250 apps.

\newpage

## Step 2: Box Plot of Distribution of Rating Scores for Each Category
```{r, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
ggplot(df, aes(x = reorder(Category, Rating, FUN = median), y = Rating)) +
  geom_boxplot(fill = "steelblue", color = "black", outlier.color = "red", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Distribution of Rating Scores by Category",
       x = "Category",
       y = "Rating") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold")) +
  coord_flip()

rating_dist <- df %>%
  group_by(Category) %>%
  summarise(
    count = n(),
    min = min(Rating, na.rm = TRUE),
    mean = mean(Rating, na.rm = TRUE),
    sd = sd(Rating, na.rm = TRUE),
    max = max(Rating, na.rm = TRUE)
  ) %>%
  arrange(desc(count))
```
From the box plot for each category above, it is clear that almost all the categories have median value greater than 4. Also, maybe due to less number of apps in comics category, it is the only category without outliers. What's more, the last four categories at bottom: Tools, Maps and Navigation, Entertainment and Dating tend to have lower median values compared to other groups, which may reflect the user dissatisfaction for those app groups.

\newpage

## Step 3: Table and Box Plot of Distribution of Number of Installs in Each Category
```{r graph, fig.pos='h', echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
install_dist <- df %>%
  group_by(Category) %>%
  summarise(
    count = n(),
    min = min(Installs, na.rm = TRUE),
    median = median(Installs, na.rm = TRUE),
    max = max(Installs, na.rm = TRUE)
  ) %>%
  arrange(desc(count))


ggplot(df, aes(x = reorder(Category, Installs, FUN = median), y = Installs)) +
  geom_boxplot(fill = "steelblue", color = "black", outlier.color = "red", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Distribution of Installs by Category",
       x = "Category",
       y = "Number of Installs") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold")) +
  coord_flip()
```
```{r table1, results='asis', table.pos='h', echo=FALSE}
top_category <- head(install_dist)

top_category %>%
  kbl(
    caption = "Summary of Installations by Category",
    col.names = c("Category", "Count", "Min Installs", "Median Installs", "Max Installs"),
    format = "latex"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "center"
  ) %>%
  column_spec(2:5, width = "3cm") %>%
  row_spec(0, bold = TRUE, color = "white", background = "#007bff")
```

From this boxplot above, in general, games tend to have the more number of installs, and the range of app installs in the game category also tend to be greater than other categories. However, there is one outlier in personalization category, which has the most number of installs across all categories.

## Step 4: Distribution of Reviews in Each Category
```{r, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
ggplot(df, aes(x = reorder(Category, Reviews, FUN = median), y = Reviews)) +
  geom_boxplot(fill = "steelblue", color = "black", outlier.color = "red", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Distribution of Reviews by Category",
       x = "Category",
       y = "Number of Reviews") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold")) +
  coord_flip()
```
As shown in the box plot above, game category still tend to have more reviews than the other category, with the most number of reviews over $4*10^7$.

## Step 5: Correlation Analysis of Numeric Variables
```{r, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
numerical_data <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numerical_data, use = "complete.obs")


corrplot(cor_matrix, method = "color", type = "full", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black",
         col = colorRampPalette(c("blue", "white", "red"))(200))
```
From the correlation plot, most of the numerical variables do not have strong correlation with each other, except the installs and review. Their correlation is 0.64, which means that there is positive relationship between those two variables. When there is more installs for the app, it tends to have more reviews.

We can also plot the relationship of those numeric variables with Rating to assess their relationships:
```{r, echo=FALSE, fig.width=10, fig.height=8, fig.align='center'}
# Create individual plots
p1 <- ggplot(numerical_data, aes(x=Reviews, y=Rating)) +
  geom_point() + ggtitle("Reviews vs Rating")

p2 <- ggplot(numerical_data, aes(x=`Size(in MB)`, y=Rating)) +
  geom_point() + ggtitle("Size vs Rating")

p3 <- ggplot(numerical_data, aes(x=Installs, y=Rating)) +
  geom_point() + ggtitle("Installs vs Rating")

p4 <- ggplot(numerical_data, aes(x=Price, y=Rating)) +
  geom_point() + ggtitle("Price vs Rating")

# Combine plots into a grid
plot_grid(p1, p2, p3, p4, ncol = 2, align = "hv")
```
Based on the scatterplot above, all the numeric variables show a similar log relationship with the response variable Rating, indicating that log transformation may be needed to transform those variables.

## Step 6: Check Association Between Categorical Variables Using Chi-Square Test
```{r, echo=FALSE, warning=FALSE}
df$Category <- as.factor(df$Category)
df$Type <- as.factor(df$Type)
df$Content.Rating <- as.factor(df$Content.Rating)
Category_vs_Type <- table(df$Category, df$Type)
Category_vs_ContentRating <- table(df$Category, df$Content.Rating)
Type_vs_ContentRating <- table(df$Type, df$Content.Rating)
chi_square_result1 <- chisq.test(Category_vs_Type)
chi_square_result2 <- chisq.test(Category_vs_ContentRating)
chi_square_result3 <- chisq.test(Type_vs_ContentRating)
print(chi_square_result1)
print(chi_square_result2)
print(chi_square_result3)
```
From the chi-square results above, since all the p-value of the tests are less than 0.05, which means that there is association between category and type, category and content rating, type and content rating.

\newpage

# Method
## Step 1: Selection of Models

Since the response variable rating is a continuous variable, logistic and multinomial models cannot be used. Also, the response variable is not count data and not discrete, so poisson and negative binomial models cannot be used. Thus, the models that I want to use are linear regression model or linear mixed effect models. First plot the distribution of rating score:
```{r, message=FALSE, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
ggplot(data = df, aes(x=Rating))+
  geom_histogram() +
  theme_minimal()
```
Since the rating score is left-skewed, I want to do some transformation to make it more symmetric. The method I would like to use is power transformation.
```{r, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
df$Rating <- df$Rating^4
ggplot(data = df, aes(x=Rating))+
  geom_histogram(binwidth = 50) +
  theme_minimal()
```
After taking rating to the power of 4, the distribution of rating score becomes symmetric. So in the later report, when I refer to the variable rating, it refers to $rating^4$.

## Step 2: Start from Null Model

The formula of **null model** is:

\begin{align}
Rating = \beta_0 \tag{0}
\end{align}

```{r, echo=FALSE}
model0 <- lm(Rating~1, data = df)
```

The model output is in the Null Model section in Appendix.

In this null model, the estimate of the intercept is just the overall mean of rating score of all the apps in the whole dataset. 

## Step 3: Building the Linear Regression Model and Transformation of Data

Building upon the null model, based on the correlation matrix and scatterplot in EDA, I will first add reviews, installs, size and price in its original form into the model:

The formula of **model 1** is:

\begin{align}
Rating = \beta_0 + \beta_1 \cdot Reviews+\beta_2 \cdot Installs+\beta_3 \cdot Size+\beta_4 \cdot Price \tag{1}
\end{align}

```{r, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
model1 <- lm(Rating~Reviews+Installs+`Size(in MB)`+Price, data = df)
plot(model1, which = 1)
plot(model1, which = 2)
```

The model output is in the Model 1 section in Appendix.

Based on the model output result, variable **Reviews and Price** have p-value less than 0.05, suggesting that they have statistically significant effect on Rating. Additionally, F-test is less than 0.05, suggesting that there is at least one variable that is statistically significant. However, based on the residuals vs fitted value plot, the points are not randomly scattered around 0, indicating that there is heteroscedasticity problem. Based on the QQ plot, there are some points that deviate from the line, but overall, the normality assumption is not violated. Since the homoscedasticity assumptions of the linear regression models is violated, the model cannot used to fit the dataset, which may explain the low adjusted R-square value.

As mentioned in EDA part, one thing I would like to try is doing log transformation on the predictor variable, and based on the residual vs fitted plot, the fitted value is concentrated in a narrow range, and this could be related to the high-skewness or low variability in one of the predictor variables:
```{r, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
par(mfrow = c(1, 3))

hist(df$Reviews, main = "Reviews", col = "skyblue", xlab = "Reviews")
hist(df$Price, main = "Price", col = "lightgreen", xlab = "Price")
hist(df$Installs, main = "Installs", col = "pink", xlab = "Installs")

par(mfrow = c(1, 1))
```
Based on the histograms above, those three numeric variables are all highly skewed, so I could do log transformation on those three variables.

Here is the formula for the **model 2**, model output in Model 2 section in Appendix, the reason to add 1 in log is to avoid some zero value for those variables because log is not defined at 0:

\begin{align}
Rating &= \beta_0 + \beta_1 \cdot log(Reviews+1)+\beta_2 \cdot log(Installs+1)+ \nonumber \\
&\beta_3 \cdot log(Size+1)+\beta_4 \cdot log(Price+1) \tag{2}
\end{align}

```{r, echo=FALSE, fig.width=6, fig.height=4, fig.align='center'}
model2 <- lm(Rating~log(Reviews+1)+log(Installs+1)+log(`Size(in MB)`+1)+log(Price+1), data = df)
plot(model2, which = 1)
plot(model2, which = 2)
```
After doing the log transformation on predictors, in the residual vs fitted plot, the residuals become more spread around 0, indicating the homoscedasticity assumption is no longer violated, and the QQ plot has less heavy-tail problem, indicating the normality of residual distribution is also no longer. What's more, the adjust r-squared value is ten times the original model, and the residual standard error also decreased, meaning that model 2 is a better model than model 1. I could add more categorical variables into the model and see if it improves the model. Since **log(Size)** is no longer significant in this model, I can consider removing it for the later model.

Here is the formula for **model 3**, model output is in the Model 3 section in Appendix:

\begin{align}
Rating &= \beta_0 + \beta_1 \cdot log(Reviews+1)+\beta_2 \cdot log(Installs+1)+ \nonumber \\
&\beta_3 \cdot TypePaid+\beta_4 \cdot log(Price+1)+ \nonumber \\
&\beta_5 \cdot Category \tag{3}
\end{align}

```{r, echo=FALSE}
model3 <- lm(Rating~log(Reviews+1)+log(Installs+1)+Type+
               log(Price+1)+Category, data = df)
```
In the formula above, because there are too many levels in Category variable, it is written as one variable with one coefficient, but in the actual model, it is treated as 32 different levels(1 level is reference) with 32 coefficients.

In model 3, residual standard error decreased and adjusted R-square increased, meaning that adding category variable increase the model performance. Let's try adding two more categorical variables.

Here is the formula for **model 4**, model output is in the Model 4 section in Appendix:

\begin{align}
Rating &= \beta_0 + \beta_1 \cdot log(Reviews+1)+\beta_2 \cdot log(Installs+1)+ \nonumber \\
&\beta_3 \cdot TypePaid+\beta_4 \cdot log(Price+1)+ \nonumber \\
&\beta_5 \cdot Category+\beta_6 \cdot Content.Rating+ \nonumber \\
&\beta_7 \cdot Last.Updated \tag{4}
\end{align}

```{r, echo=FALSE}
model4 <- lm(Rating~log(Reviews+1)+log(Installs+1)+Type+
               log(Price+1)+Category+Content.Rating+Last.Updated, 
             data = df)
```
Similar to model 3, in the actual model, each level of categorical variable get a coefficient, for the sake of easy writing, they are each treated as one single variable in the formula.

Based on the model4 output result, the residual standard error and adjusted r-squared all get further improvement.

After getting those five models with only fixed effects, I can use MSE to determine which model is the best model and compare it with the null model:
```{r, echo=FALSE}
# Compute predictions for each model
pred_0 <- predict(model0, type = "response")
pred_1 <- predict(model1, type = "response")
pred_2 <- predict(model2, type = "response")
pred_3 <- predict(model3, type = "response")
pred_4 <- predict(model4, type = "response")

# Compute residuals
residuals_0 <- df$Rating - pred_0
residuals_1 <- df$Rating - pred_1
residuals_2 <- df$Rating - pred_2
residuals_3 <- df$Rating - pred_3
residuals_4 <- df$Rating - pred_4

# Compute MSE for each model
mse_0 <- mean(residuals_0^2)
mse_1 <- mean(residuals_1^2)
mse_2 <- mean(residuals_2^2)
mse_3 <- mean(residuals_3^2)
mse_4 <- mean(residuals_4^2)
```

\newpage

```{r, echo=FALSE}
anova(model0, model4, test = "Chisq")
```
```{r table2, results='asis', table.pos='H', echo=FALSE}
mse_table <- data.frame(
  Model = c("Null Model", "Model 1", "Model 2", "Model 3", "Model 4"),
  MSE = c(mse_0, mse_1, mse_2, mse_3, mse_4)
)

mse_table %>%
  kbl(caption = "Mean Squared Error (MSE) for Models", 
      col.names = c("Model Name", "MSE Value"), 
      digits = 2) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"), 
    full_width = FALSE, 
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#007bff")
```
By comparing their MSE and chi-square test between model 4 and null model, model 4 is the best model, which has the lowest MSE, residual standard error and highest adjusted R-squared.

## Step 4: Fit the No Pooling, Partial Pooling and Complete Pooling Model

Based on the model fitting result above, some of the categories of apps are statistically significant, also, in the EDA part, each group's variability in rating score is different. Thus, I could try to treat Category variable as a group variable and fit a LMM on the data. The model output for all three models can be found in the appendix section.

Let's start with the **no pooling model**, which is similar to model 4, here is the formula:

\begin{align}
Rating &= \beta_1 \cdot log(Reviews+1)+\beta_2 \cdot log(Installs+1)+ \nonumber \\
&\beta_3 \cdot TypePaid+\beta_4 \cdot log(Price+1)+ \nonumber \\
&\beta_5 \cdot Category+\beta_6 \cdot Content.Rating+ \nonumber \\
&\beta_7 \cdot Last.Updated \tag{5}
\end{align}

```{r, echo=FALSE}
model_nopool <- lm(Rating~log(Reviews+1)+log(Installs+1)+Type+
               log(Price+1)+
               Category+Content.Rating+Last.Updated-1, 
             data = df)
```
In this no pooling model, each category is treated as a separate variable in the model and has its own coefficients. Also, the adjusted R-squared increased a lot compared to model 4.

Now let's fit the **partial pooling model**, here is the formula:

\begin{align}
Rating &= \beta_1 \cdot log(Reviews+1)+\beta_2 \cdot log(Installs+1)+ \nonumber \\
&\beta_3 \cdot TypePaid+\beta_4 \cdot log(Price+1)+\beta_5 \cdot Content.Rating+ \nonumber \\
&\beta_6 \cdot Last.Updated+(1|Category) \tag{6}
\end{align}

```{r, echo=FALSE}
model_partialpool <- lmer(Rating~log(Reviews+1)+log(Installs+1)+Type+
               log(Price+1)+Content.Rating+Last.Updated+
                 (1|Category), 
                   data = df)
```
Based on the output of the partial pooling model, the variance of app rating score of each Category group is very small compared to overall variance of app rating score across all Category groups. Also, compared with no pooling model, the residual standard deviation is similar compared with no pooling model, indicating that the random effect may not be necessary.

Another option is to try **complete pooling** and completely ignore the Category Variable, here is the formula:

\begin{align}
Rating &= \beta_1 \cdot log(Reviews+1)+\beta_2 \cdot log(Installs+1)+ \nonumber \\
&\beta_3 \cdot TypePaid+\beta_4 \cdot log(Price+1)+\beta_5 \cdot Content.Rating+ \nonumber \\
&\beta_6 \cdot Last.Updated \tag{7}
\end{align}

```{r, echo=FALSE}
model_completepool <- lm(Rating~log(Reviews+1)+log(Installs+1)+Type+
               log(Price+1)+Content.Rating+Last.Updated, 
                   data = df)
```
In this model, compared with previous two models, the residual standard error increased, and its adjusted R-squared is lower than the no pooling model, indicating that the model may be worse than the previous two.

```{r table3, results='asis', table.pos='H', echo=FALSE}
# Compute predictions for each model
pred_nopool <- predict(model_nopool, type = "response")
pred_partialpool <- predict(model_partialpool, type = "response")
pred_completepool <- predict(model_completepool, type = "response")

# Compute residuals
residuals_nopool <- df$Rating - pred_nopool
residuals_partialpool <- df$Rating - pred_partialpool
residuals_completepool <- df$Rating - pred_completepool

# Compute MSE for each model
mse_nopool <- mean(residuals_nopool^2)
mse_partialpool <- mean(residuals_partialpool^2)
mse_completepool <- mean(residuals_completepool^2)

mse_table <- data.frame(
  Model = c("No Pooling Model", "Partial Pooling Model", "Complete Pooling Model"),
  MSE = c(mse_nopool, mse_partialpool, mse_completepool)
)

mse_table %>%
  kbl(caption = "Mean Squared Error (MSE) for Models", 
      col.names = c("Model Name", "MSE Value"), 
      digits = 2) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"), 
    full_width = FALSE, 
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#007bff")
```
To further compare those three model, MSE could be calculated. Based on the MSE table above, the model with no pooling gets the lowest MSE, which means that it is the best model among the three models.

\newpage

## Step 5: Compare the No Pooling Model and Model 4
```{r, echo=FALSE, warning=FALSE}
aic_model4 <- AIC(model4)
aic_nopool <- AIC(model_nopool)

pred_nopool <- predict(model_nopool, type = "response")
pred_model4 <- predict(model4, type = "response")
residuals_nopool <- df$Rating - pred_nopool
residuals_model4 <- df$Rating - pred_model4
mse_nopool <- mean(residuals_nopool^2)
mse_model4 <- mean(residuals_model4^2)
```

```{r table4, results='asis', table.pos='H', echo=FALSE}
mse_table <- data.frame(
  Model = c("No Pooling Model", "Model 4"),
  AIC = c(aic_nopool, aic_model4),
  MSE = c(mse_nopool, mse_model4)
)

mse_table %>%
  kbl(caption = "Comparison of Model 4 and No Pooling Model", 
      col.names = c("Model Name", "AIC Value", "MSE Value"), 
      digits = 2) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"), 
    full_width = FALSE, 
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#007bff")
```

Based on the results above, no pooling model gets very similar result with model 4 for both AIC and MSE because no pooling model is just model 4 removing the intercept term. For easier model interpretation, I would choose model 4 for further analysis, but with high AIC value and lower MSE value, this could indicate the overfitting problem with model 4.

## Step 6: Add Interaction Term into Model 4

Since based on the model 4 output, **Type and log(Reviews)** are statistically significant, I could try adding the interaction term between these two, here is the formula for the model, the model output can be found in the appendix section:

\begin{align}
Rating &= \beta_1 \cdot log(Reviews+1)+\beta_2 \cdot log(Installs+1)+ \nonumber \\
&\beta_3 \cdot TypePaid+\beta_4 \cdot log(Price+1)+\beta_5 \cdot Content.Rating+ \nonumber \\
&\beta_6 \cdot Last.Updated+\beta_7 \cdot Category+\beta_8 \cdot log(Reviews+1) \cdot TypePaid \tag{8}
\end{align}

```{r, echo=FALSE}
model4_interaction <- lm(Rating~log(Installs+1)+
               log(Price+1)+Type*log(Reviews+1)+
                 Content.Rating+Last.Updated, 
             data = df)
anova(model4_interaction, model4, test = "Chisq")
```
Based on the anova table result above, the chi-square test shows that model 2, which is the model without interaction term, is better. Thus, the additional interaction term between log(Reviews+1) and Type is not needed.

## Step 7: Interpretation of Model 4 Selected

Now, after choosing the best model, I will try to interpret this model:

Based on the model output, the residual's median is 0, and min, max, first quantile and third quantile are all close to each other, meaning that the residual's distribution is symmetric.

To interpret the coefficient, since there are many variables, especially in category, content rating and last updated, I will only select one of the level from each variable to interpret.

For the coefficient of reviews, one unit increase in $log(Reviews + 1)$, the ratings(which is the 4th power of original rating score) is expected to increase by 30.91, keeping all other variables constant. This shows the positive correlation between reviews and rating score.

For the coefficient of installs, one unit increase in $log(Installs + 1)$, the ratings is expected to decrease by 30.2, keeping all other variables constant. This shows the negative correlation between installs and rating score.

The interpretation of coefficient of log(price) is similar to previous ones, and it also has negative correlation with the rating scores.

For coefficient of Type, the average rating score for paid app is 20.21 higher compared to free app, keeping all other variables constant.

For category level auto and vehicles, the coefficient is -58.6, which shows that the average rating score for apps belonging to auto and vehicles group is 58.6 lower compared to the apps in arts and design group(reference group), keeping all other variables constant.

For content rating level everyone, the coefficient is -42.1, which shows that the average rating score for apps with content rating as everyone is 42.1 lower compared to the apps with content rating as Adults(reference group), keeping all other variables constant.

For the last updated date April 12th, 2016, the coefficient is 204, which shows that the average rating score for apps that last updated at April 12th, 2016 is 204 higher compared to the apps last updated at April 1st 2016(reference group), keeping all other variables constant.

The residual standard error is 112.9, which is the lowest compared to other models that I fit. The adjust R-square is 0.1482, which indicated that the model can only explain 14.82% variability in the response variable, and although this is the best model I get, the model still does not fit the data well. The F-statistic has p-value less than 0.05, showing that there is at least one variable that is statistically significant, and the model is better than the null model.

The important predictors of rating score according to my model are reviews, installs, type, price, category and last updated date.

# Discussion

To validate the result of my model, I found three research paper that related to rating score prediction in Google Play Store. 

Based on the paper by S Shashank et.al[2], they tried to predict the rating score using machine learning algorithm. The paper used the same dataset as I do in this report. Compared to my approach, the author do a more detailed EDA. Instead of focusing on the category of apps, they pay more attention on whether the app is free or paid and find out the difference in rating score between free app group and paid app group. In the method part, the authors applied five techniques trying to find the important variables related to rating score: random forest, support vector regression, linear regression, k-nearest neighbors and k-means clustering. As a results, the k-means neighbors achieves the best result, which has 92% prediction accuracy, and the author concludes that **size, type, price, content rating and genre** are variables strongly correlated with the rating score. The author's result is consistent with my result, and those are also statistically significant variables in my linear regression model, except I used category instead of genre and exclude size from my model. However, after reading the paper, I find out there could be bias related to the rating score because higher ratings given by users potentially attract several new users disproportionately, and people tend to only use apps with high rating score, leading to more reviews of the app. Additionally, many people don't like writing reviews for the apps no matter they like the app or not, there are also people writing negative reviews but give very positive rating scores, so for some apps, the rating score may not reflect their true quality. 

Another research paper I found was written by Min-Kyo Seo et.al[3]. The main purpose of this paper is to investigate the predictors and main determinants of consumersâ€™ ratings of mobile applications in the Google Play Store. The author also tried to extend their model into a sentimental analysis and aim to review polarity and subjectivity on the application rating. In the data preprocessing part, they used sentiment analysis based on the users reviews from Google Play Store and created new variables polarity and subjectivity and merge them into the original Google Play dataset. In the method part, there were four models they used: multiple linear regression, regression tree, random forest tree and neural network. Based on the model result, neural network model gives the lowest RMSE result, and the important variables are **price, installs and reviews**, and polarity and subjectivity of reviews are less critical. Those variables are also included in my model, one difference is that install is positively correlated with ratings based on the author's results, but install is negative correlated with rating score based on my model's results.

The last research paper I found is by Jayanth. P et.al[4], and the main purpose of research paper is to predict the rating score using the comprehensive Google Play Store dataset similar to the one that I used. The method that the author used are: lasso regression model, ridge regression model, gradient boosting, XGBoost and CATBoost. Based on the result, CATBoost method provides the lowest MAE, MSE, RMSE and highest $R^2$ value, but the top features that the author find that can influence the rating score is different from my results, the author find that **reviews, last updated and android version** are the most critical variables. In my models, android version is not included, and most of the levels of last updated are not statistically significant.

Based on the literature review above, the next step for my analysis could be try to implement tree model or deep learning model to further improve my current linear regression model.

\newpage

# Appendix
## Model Output
### Null Model
```{r, echo=FALSE}
summary(model0)
```

### Model 1
```{r, echo=FALSE}
summary(model1)
```

### Model 2
```{r, echo=FALSE}
summary(model2)
```

### Model 3
```{r, echo=FALSE}
summary(model3)
```

### Model 4
```{r, echo=FALSE}
summary(model4)
```

### No Pooling Model
```{r, echo=FALSE}
summary(model_nopool)
```

### Partial Pooling Model
```{r, echo=FALSE}
summary(model_partialpool)
```

### Complete Pooling Model
```{r, echo=FALSE}
summary(model_completepool)
```

### Interaction Model Output
```{r, echo=FALSE}
summary(model4_interaction)
```


## Reference

[1] J. P, A. Nagam, P. Undavalli, P. P, V. P. K. S and V. K. K. K, "Leveraging CAT Boost for Enhanced Prediction of App Ratings in the Google Play Store," 2024 Second International Conference on Advances in Information Technology (ICAIT), Chikkamagaluru, Karnataka, India, 2024, pp. 1-6, doi: 10.1109/ICAIT61638.2024.10690600.

[2] Statista (2024) Google Play Store - Statistics & Facts. Available at: https://www.statista.com/topics/9929/google-play-store/#topicOverview (Accessed: 27 November 2024).

[3] S Shashank and Brahma Naidu, "Google play store apps-data analysis and ratings prediction", International Research Journal of Engineering and Technology, vol. 7.12, pp. 265-274, 2020.

[4] Seo, M.K., Yang, O.S. and Yang, Y.H., 2020. Global Big Data Analysis Exploring the Determinants of Application Ratings: Evidence from the Google Play Store. Journal of Korea Trade, 24(7), pp.1-28.








