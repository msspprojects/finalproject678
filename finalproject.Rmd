---
title: "Google Play Store Data Analysis Report"
author: "Suheng Yao"
date: "2024-11-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(corrplot)
library(MASS)
options(max.print = 30)
library(lme4)
library(glmmTMB)
```


```{r, echo=FALSE}
# Load in The Dataset
df <- read.csv("google_play_store_dataset.csv")
```
# Abstract

# Introduction

Google Play Store has always been the most popular and largest app store for Android phone users across the world. Since it is pre-installed on supporting Android devices, and the operative system holds over 70 percent of the global market, Google Play Store becomes the default app hub for most of the Android users worldwide(). Up until the second quarter of 2024, there are 2.26 million apps available in the store().

In this project, the main question of interest is: What are the important factors that can affect the apps ratings? The reasons why I am interested in this question are divided into two part: first of all, since I use Google Play to install apps every day, I am always curious about what kind of apps can have good ratings scores and why some apps I find easy to use get low rating score. Secondly, by doing this analysis, if I want to build apps to publish on Google Play in the future, then I will know which essential factors to focus on or the popular fields to go into.

This project report will be divided into four parts: methods section will talk about model selection and model building; results section will talk about the important findings after fitting the models; discussion section will further analyze those findings and talk about the next steps for the analysis; appendix is the last part, which will include EDA and initial understandings of the data.

# Appendix
## Data Cleaning
```{r, echo=FALSE}
glimpse(df)
```
Looking at the structure of the dataset, there are in total 10,841 observations and 13 variables related to each app. The response variable is **Rating**, which is a continuous variable. By looking at the column names of the data, "Category" and "Genre" seem to have similar data, I can print out the unique values in those two columns:
```{r, echo=FALSE}
print(unique(df$Category))
print(unique(df$Genres))
```
Based on the printed results above, "Genre" is just the more detailed classification of "Category", since in this project, I mostly focus on the general groups of apps, I can remove the "Genre" column and change the Category name to lower case.
```{r, echo=FALSE}
df <- df %>%
  dplyr::select(-Genres)

df$Category <- tolower(df$Category)
print(head(df))
```
Also, from the distinct values of category, there is a category called "1.9", which does not make sense. Since there is only one record of data with category "1.9", I can just remove this record from the dataset.
```{r, echo=FALSE}
df <- df %>%
  filter(Category != 1.9)
```
Now, let's check if there are any duplicated apps in the dataset:
```{r, echo=FALSE}
print(sum(duplicated(df$App)))
```
From the analysis above, there are 1181 duplicated apps in the dataset, to make further analysis easier, I will just keep the first occurrence of each app record.
```{r, echo=FALSE}
df <- df %>%
  distinct(App, .keep_all = TRUE)
```
Now, I need to check if there are NA values in the dataset:
```{r, echo=FALSE}
summary(df)
```
From the result shown above, in the response variable "Rating", there are 1463 missing values. To maintain the size of the data, I will fill in those missing values using median values of the "Rating" column.
```{r, echo=FALSE}
df$Rating[is.na(df$Rating)] <- median(df$Rating, na.rm = TRUE)
summary(df)
```
Since it makes more sense to talk about installs, reviews, size and price in numerical values, I will change those variables from categorical to numeric:
```{r, echo=FALSE, warning=FALSE}
df$Reviews <- as.numeric(df$Reviews)
df$Size <- as.numeric(gsub("M", "", df$Size))
colnames(df)[colnames(df) == "Size"] <- "Size(in MB)"
df$Price <- as.numeric(gsub("\\$", "", df$Price))
summary(df)
```
Since Most of the NA values in Size are related to "Varies with Device" value in the original dataset, and in this project, I want to mostly focus on the app with fixed size, I will just remove those app records with varied app sizes.
```{r, echo=FALSE}
df <- df %>%
  filter(!is.na(`Size(in MB)`))
```

Transforming Installs is a more complex matter, I will solve this problem differently. First check the distinct values in the variable "Installs":
```{r, echo=FALSE}
print(unique(df$Installs))
```
To convert those values into numeric values and avoid duplicate values, take "500+" as an example, I will change it to a value between 500 to 1000 because "1000+" will be on a different level.
```{r, echo=FALSE}
df$Installs <- as.numeric(gsub("\\+", "", gsub(",", "", df$Installs)))

generate_installs <- function(x) {
  if (x == 0) {
    return(0)
  } else if (x == 1) {
    lower_bound <- x
    upper_bound <- 10
  } else if (x == 5) {
    lower_bound <- x
    upper_bound <- 10
  } else if (x == 10) {
    lower_bound <- x
    upper_bound <- 50
  } else if (x == 50) {
    lower_bound <- x
    upper_bound <- 100
  } else if (x == 100) {
    lower_bound <- x
    upper_bound <- 500
  } else if (x == 500) {
    lower_bound <- x
    upper_bound <- 1000
  } else if (x == 1000) {
    lower_bound <- x
    upper_bound <- 5000
  } else if (x == 5000) {
    lower_bound <- x
    upper_bound <- 10000
  } else if (x == 10000) {
    lower_bound <- x
    upper_bound <- 50000
  } else if (x == 50000) {
    lower_bound <- x
    upper_bound <- 100000
  } else if (x == 100000) {
    lower_bound <- x
    upper_bound <- 500000
  } else if (x == 500000) {
    lower_bound <- x
    upper_bound <- 1000000
  } else if (x == 1000000) {
    lower_bound <- x
    upper_bound <- 5000000
  } else if (x == 5000000) {
    lower_bound <- x
    upper_bound <- 10000000
  } else if (x == 10000000) {
    lower_bound <- x
    upper_bound <- 50000000
  } else if (x == 50000000) {
    lower_bound <- x
    upper_bound <- 100000000
  } else if (x == 100000000) {
    lower_bound <- x
    upper_bound <- 500000000
  } else if (x == 500000000) {
    lower_bound <- x
    upper_bound <- 1000000000
  } else if (x == 1000000000) {
    lower_bound <- x
    upper_bound <- 2000000000
  } else {
    lower_bound <- x
    upper_bound <- x * 10  # Fallback case if something is missed
  }
    return(round(runif(1, min = lower_bound, max = upper_bound)))
}

df$Installs <- sapply(df$Installs, generate_installs)
summary(df)
```
After those data cleaning is done, I will start doing EDA.

## EDA
### Step 1: Bar Plot Counting Distinct Apps in Each Category
```{r, echo=FALSE}
category_counts <- df %>%
  group_by(Category) %>%
  summarise(App_Count = n_distinct(App)) %>%
  arrange(desc(App_Count))

ggplot(category_counts, aes(x = reorder(Category, -App_Count), y = App_Count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(title = "Number of Distinct Apps in Each Category",
       x = "Category",
       y = "Number of Distinct Apps") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
From the count of distinct apps, it is clear that category family gets the most number of apps, with more than 1500 apps in the category. The second most category is game, which is only half the number of the family category, and the third most is the tools category. The category with the least number of distinct apps is comics, with only less than 250 apps.

### Step 2: Box Plot of Distribution of Rating Scores for Each Category
```{r, echo=FALSE}
ggplot(df, aes(x = reorder(Category, Rating, FUN = median), y = Rating)) +
  geom_boxplot(fill = "steelblue", color = "black", outlier.color = "red", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Distribution of Rating Scores by Category",
       x = "Category",
       y = "Rating") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold")) +
  coord_flip()

rating_dist <- df %>%
  group_by(Category) %>%
  summarise(
    count = n(),
    min = min(Rating, na.rm = TRUE),
    mean = mean(Rating, na.rm = TRUE),
    sd = sd(Rating, na.rm = TRUE),
    max = max(Rating, na.rm = TRUE)
  ) %>%
  arrange(desc(count))
```
From the box plot for each category above, it is clear that almost all the categories have median value greater than 4. Also, maybe due to less number of apps in comics category, it is the only category without outliers. What's more, the last four categories at bottom: Tools, Maps and Navigation, Entertainment and Dating tend to have lower median values compared to other groups, which may reflect the user dissatisfaction for those app groups.

### Step 3: Table and Box Plot of Distribution of Number of Installs in Each Category
```{r, echo=FALSE}
install_dist <- df %>%
  group_by(Category) %>%
  summarise(
    count = n(),
    min = min(Installs, na.rm = TRUE),
    median = median(Installs, na.rm = TRUE),
    max = max(Installs, na.rm = TRUE)
  ) %>%
  arrange(desc(count))

print(install_dist)
df <- df %>%
  mutate(Installs = as.numeric(scale(Installs)))


ggplot(df, aes(x = reorder(Category, Installs, FUN = median), y = Installs)) +
  geom_boxplot(fill = "steelblue", color = "black", outlier.color = "red", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Distribution of Standardized Installs by Category",
       x = "Category",
       y = "Standardized Installs") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold")) +
  coord_flip()
```
The table is based on original values of installs, and to make every numeric on a similar scale, I try to standardize the install variable, and the box plot is based on standardized install values. From this boxplot above, in general, games tend to have the more number of installs, and the range of app installs in the game category also tend to be greater than other categories. However, there is one outlier in personalization category, which has over $1.5*10^9$ installs.

### Step 4: Distribution of Reviews in Each Category
```{r, echo=FALSE}
df <- df %>%
  mutate(Reviews = as.numeric(scale(Reviews)))

ggplot(df, aes(x = reorder(Category, Reviews, FUN = median), y = Reviews)) +
  geom_boxplot(fill = "steelblue", color = "black", outlier.color = "red", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Distribution of Standardized Reviews by Category",
       x = "Category",
       y = "Standardized Reviews") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold")) +
  coord_flip()
```
Similar to installs, I standardize the review variable. As shown in the box plot above, game category still tend to have more reviews than the other category, with the most number of reviews over $4*10^7$.

### Step 5: Correlation Analysis of Numeric Variables
```{r, echo=FALSE}
numerical_data <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numerical_data, use = "complete.obs")


corrplot(cor_matrix, method = "color", type = "full", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black",
         col = colorRampPalette(c("blue", "white", "red"))(200))
```
From the correlation plot, most of the numerical variables do not have strong correlation with each other, except the installs and review. Their correlation is 0.63, which means that there is positive relationship between those two variables. When there is more installs for the app, it tends to have more reviews.

### Step 6: Check Association Between Categorical Variables Using Chi-Square Test
```{r, echo=FALSE, warning=FALSE}
df$Category <- as.factor(df$Category)
df$Type <- as.factor(df$Type)
df$Content.Rating <- as.factor(df$Content.Rating)
Category_vs_Type <- table(df$Category, df$Type)
Category_vs_Rating <- table(df$Category, df$Rating)
Type_vs_Rating <- table(df$Type, df$Rating)
chi_square_result1 <- chisq.test(Category_vs_Type)
chi_square_result2 <- chisq.test(Category_vs_Rating)
chi_square_result3 <- chisq.test(Type_vs_Rating)
print(chi_square_result1)
print(chi_square_result2)
print(chi_square_result3)
```
From the chi-square results above, since all the p-value of the tests are less than 0.05, which means that there is association between category and type, category and rating and type and rating pair.

## Model Diagnostics
### Step 1: Selection of Models

Since the response variable rating is a continuous variable, logistic and multinomial models cannot be used. Also, the response variable is not count data and not discrete, so poisson and negative binomial models cannot be used. Thus, the models that I want to use are linear regression model or linear mixed effect models.

### Step 2: Start from Null Model
```{r, echo=FALSE}
model0 <- lm(Rating~1, data = df)
summary(model0)
```
In this null model, the estimate of the intercept is just the overall mean of rating score of all the apps in the whole dataset. Also, the distribution of residuals is not symmetric, suggesting that the model does not fit the data well. Building upon the null model, I will add some variables into the model:
```{r, echo=FALSE}
model1 <- lm(Rating~Reviews+Installs+Type+Price, data = df)
summary(model1)
plot(model1, which = 1)
plot(model1, which = 2)
```
Based on the model result, variable **Reviews, Type and Price** have p-value less than 0.05, suggesting that they have statistically significant effect on Rating. Additionally, F-test is less than 0.05, suggesting that there is at least one variable that is statistically significant. However, based on the residuals vs fitted value plot, the points are not randomly scattered around 0, indicating that there is heterosedasticity problem. Also, based on the QQ plot, there is heavy-tail problem, indicating that the residuals are not normally distributed. Based on the violations of the two important assumptions of the linear regression models, the model cannot used to fit the dataset, which may explain the low adjusted R-square value.

To tackle this non-normal residual problem, one thing I would like to try is the Robust Regression Model because it allows the residuals to follow t-distribution, which is skewed.
```{r, echo=FALSE}
robust_model <- rlm(Rating ~ Reviews + Type + Price, data = df)
summary(robust_model)
plot(robust_model, which = 1)
plot(robust_model, which = 2)
```
Although the residual standard error does become smaller, the QQ plot still gets the heavy-tail problem, meaning that the residuals are still not normally distributed. 

### Step 3: Fit the Gamma Regression Model

We can plot the distribution of ratings across all the apps:
```{r, message=FALSE}
ggplot(data = df, aes(x=Rating))+
  geom_histogram() +
  theme_minimal()
```
From the histogram above, since the distribution of Rating is obviously left-skewed, another model I can try is the Gamma Regression Model in GLM:
```{r, echo=TRUE}
model_gamma1 <- glm(Rating ~ Reviews + Type + Price, 
                   family = Gamma(link = "log"), 
                   data = df)
summary(model_gamma1)
```
Since the residual deviance is close to null deviance, the model did not fit the model well. I would need to add more variables:
```{r}
model_gamma2 <- glm(Rating ~ Reviews + Type + Price + Category, 
                   family = Gamma(link = "log"), 
                   data = df)
summary(model_gamma2)
```

```{r, echo=FALSE}
model_gamma3 <- glm(Rating ~ Reviews + Type + Price + 
                     Category + Last.Updated, 
                   family = Gamma(link = "log"), 
                   data = df)

null_deviance <- model_gamma3$null.deviance
print(paste("Null Deviance:", null_deviance))

residual_deviance <- model_gamma3$deviance
print(paste("Residual Deviance:", residual_deviance))

aic_value <- AIC(model_gamma3)
print(paste("AIC:", aic_value))
```
```{r, echo=FALSE}
model_gamma4 <- glm(Rating ~ Reviews + Type + Price + 
                     Category + Android.Ver, 
                   family = Gamma(link = "log"), 
                   data = df)

null_deviance <- model_gamma4$null.deviance
print(paste("Null Deviance:", null_deviance))

residual_deviance <- model_gamma4$deviance
print(paste("Residual Deviance:", residual_deviance))

aic_value <- AIC(model_gamma4)
print(paste("AIC:", aic_value))
```
```{r, echo=FALSE}
model_gamma5 <- glm(Rating ~ Reviews + Type + Price + Category + Last.Updated + Android.Ver, 
                   family = Gamma(link = "log"), 
                   data = df)

null_deviance <- model_gamma5$null.deviance
print(paste("Null Deviance:", null_deviance))

residual_deviance <- model_gamma5$deviance
print(paste("Residual Deviance:", residual_deviance))

aic_value <- AIC(model_gamma5)
print(paste("AIC:", aic_value))
```
Based on the five gamma models fitted above, by comparing their difference between null deviance and residual deviance and AIC value, model_gamma5 should be the best model compared with other four models. Although it gets a higher AIC value, it has a near 20% decrease from its null deviance to residual deviance.

### Step 4: Fit the Multi-level Model

Based on the model fitting result above, some of the categories of apps are statistically significant, also, in the EDA part, each group's variability in rating score is different. Thus, I could try to treat Category variable as a group variable and fit a GLMM on the data.

Let's start with the no pooling model, which is similar to model 5:
```{r, echo=FALSE}
model_nopool <- glm(Rating ~ Reviews + Type + Price + Category - 1, 
                   family = Gamma(link = "log"), 
                   data = df)

summary(model_nopool)
```
In this no pooling model, each category is treated as a separate variable in the model and has its own coefficients. Also, the residual deviance decreases a lot from null deviance, indicating a better fit.

Now let's fit the partial pooling model:
```{r, echo=FALSE}
df$Price <- scale(df$Price)
model_partialpool <- glmer(Rating ~ Reviews + Type + Price + (1|Category), 
                   family = Gamma(link = "log"), 
                   data = df)

summary(model_partialpool)
```
Based on the output of the partial pooling model, the variance of app rating score of each Category group is very small compared to overall variance of app rating score across all Category groups, which means that there is already little variability in the mean rating score of each app group, and the random effect of Category groups does not explain much variability in the Rating score.

Another option is to try complete pooling and completely ignore the Category Variable:
```{r, echo=FALSE}
model_completepool <- glm(Rating ~ Reviews + Type + Price,
                   family = Gamma(link = "log"), 
                   data = df)
summary(model_completepool)
```
In this model, the residual deviance and null deviance are very close, indicating that the model did not fit the data well.

To further compare those three model, MSE could be calculated:
```{r, echo=FALSE}
# Compute predictions for each model
pred_nopool <- predict(model_nopool, type = "response")
pred_partialpool <- predict(model_partialpool, type = "response")
pred_completepool <- predict(model_completepool, type = "response")

# Compute residuals
residuals_nopool <- df$Rating - pred_nopool
residuals_partialpool <- df$Rating - pred_partialpool
residuals_completepool <- df$Rating - pred_completepool

# Compute MSE for each model
mse_nopool <- mean(residuals_nopool^2)
mse_partialpool <- mean(residuals_partialpool^2)
mse_completepool <- mean(residuals_completepool^2)

# Print MSE values
cat("MSE (No Pooling):", mse_nopool, "\n")
cat("MSE (Partial Pooling):", mse_partialpool, "\n")
cat("MSE (Complete Pooling):", mse_completepool, "\n")

```
Based on the MSE calculated, the model with no pooling gets the lowest MSE, which means that it is the best model among the three models.

### Step 5: Compare the No Pooling Model and Model 5

To compare the performance of those two models, we can first
```{r, echo=FALSE, warning=FALSE}
aic_model5 <- AIC(model_gamma5)
aic_nopool <- AIC(model_nopool)

pred_nopool <- predict(model_nopool, type = "response")
pred_model5 <- predict(model_gamma5, type = "response")
residuals_nopool <- df$Rating - pred_nopool
residuals_model5 <- df$Rating - pred_model5
mse_nopool <- mean(residuals_nopool^2)
mse_model5 <- mean(residuals_model5^2)


cat("AIC (model5):", aic_model5, "\n")
cat("MSE (model5):", mse_model5, "\n")
cat("AIC (No Pooling):", aic_nopool, "\n")
cat("MSE (No Pooling):", mse_nopool, "\n")
```
Based on the results above, model5 gets lower MSE value but higher AIC value compared to no pooling model. In order to achieve better accuracy in prediction, I would choose model5 for further analysis.

### Step 6: Add Interaction Term into Model 5

```{r, echo=FALSE}
summary(model_gamma5)
```
Since based on the model output above, reviews, type and price are all statistically significant, I could try adding the interaction term between any of those two:
```{r, echo=FALSE}
df$Price <- as.numeric(df$Price)
model5_interaction <- glm(formula = Rating ~ Reviews * Type + Price + 
                            Category + Last.Updated + 
                            Android.Ver, 
                          family = Gamma(link = "log"), data = df)
summary(model5_interaction)

coeff_table <- summary(model5_interaction)$coefficients
coeff_table[grep(":", rownames(coeff_table)), ]
anova(model5_interaction, model_gamma5, test = "Chisq")
```
Based on the model output result and anova test result above, the interaction term is not statistically significant, and adding the interaction term does not improve the model performance based on the p-value in the anova table. Thus, the additional interaction term between Reviews and Type is not needed.




